{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tristandealwis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tristandealwis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Regular Expressions\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# from ngram import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"listings.csv\", usecols=['id', 'name', 'description', 'neighbourhood_cleansed', 'zipcode', 'property_type', 'room_type', \n",
    "                                            'price', 'availability_365'])\n",
    "listings = listings.rename(columns={'id': 'listing_id'})\n",
    "\n",
    "reviews = pd.read_csv(\"reviews.csv\", usecols=['listing_id', 'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(listings, reviews, on='listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df['comments'][1:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'best', 'thing', 'about', 'this', 'place', 'is', 'the', 'location', 'you', 're', 'right', 'in', 'the', 'middle', 'of', 'clinton', 'hill', 'with', 'easy', 'access', 'to', 'restaurants', 'on', 'franklin', 'avenue', 'and', 'in', 'fort', 'greene', 'you', 're', 'also', 'close', 'to', 'the', 'and', 'trains', 'so', 'transportation', 'is', 'easy', 'the', 'space', 'is', 'clean', 'and', 'neat', 'if', 'spare', 'it', 'really', 'is', 'much', 'more', 'of', 'one', 'floor', 'apartment', 'than', 'private', 'room', 'as', 'you', 'will', 'only', 'have', 'interactions', 'with', 'the', 'family', 'on', 'the', 'shared', 'stairs', 'and', 'stoop', 'there', 'is', 'however', 'no', 'door', 'at', 'the', 'top', 'of', 'the', 'stairs', 'so', 'sounds', 'travel', 'from', 'the', 'rest', 'of', 'the', 'house', 'lisa_roxanne', 'was', 'kind', 'and', 'accommodating', 'and', 'the', 'children', 'are', 'adorable']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1     The best thing about this place is the locatio...\\n2     LisaRoxanne was friendly and helpful. The room...\\n3     LisaRoxanne is very nice and communicative. Th...\\n4     Lisa was a very pleasant and nice host that ac...\\n5     Lisa était sympa, toujours prête à aider. L'ét...\\n6     The reservation was canceled 5 days before arr...\\n7     We were greeted by Lisa's husband at the door....\\n8     Lisa and her family were warm and welcoming ho...\\n9     The place was great. Lisaroxanne was a great h...\\n10    Bien que la chambre soit défraichi, nous avons...\\n11    LisaRoxanne met us out on the sidewalk, and wa...\\n12    She is so wonder full. We got friendly and hom...\\n13    LisaRoxanne and her family are some of the mos...\\n14    Lisa was great! Communication prior to the sta...\\n15    Han sido unos días muy agradables. La casa est...\\n16    LisaRoxanne is very kind and accommodative hos...\\n17    Nice apartment, good location. Very near to Su...\\n18    We had a very good time at LisaRoxannes house....\\n19    Our stay at Lisa's was just perfect. The bedro...\\n20    LisaRoxanne was great host. She communicates w...\\n21    Alla fine tutto è bene ciò che finisce bene......\\n22    LisaRoxanne is a great host! We came about an ...\\n23    Great place to stay in a great location with f...\\n24    La descripción del departamento es absolutamen...\\n25    Je suis très heureux de mon séjour: accueil ch...\\n26    LisaRoxanne was very responsive and accessible...\\n27    Neighborhood was safe, close to subway, space ...\\n28    I quickly felt at home in Lisa Roxanne's home....\\n29    We enjoyed our brief stay. Lisa was wonderful,...\\n30    We enjoyed staying at Lisa Roxanne's place. Th...\\n31    Arrived Dec 24th around 4:30pm which we had ju...\\n32    This was my first time using Airbnb and we had...\\n33    The location is great, 6 blocks to two differe...\\n34    Had a wonderful time, very cosy and warm compa...\\n35    This good spot is accurately described by a wo...\\n36    My boyfriend and I stayed at LisaRoxanne's pla...\\n37    LisaRoxanne and her husband were great hosts! ...\\n38    I had a relaxed stay at LisaRoxanne place whic...\\n39    The apartment was just as described, clean, pr...\\n40    Great host!  Great place and plenty of space. ...\\n41    LisaRox was a fantastic host and both her and ...\\n42    Estuvo todo perfecto! El barrio es muy lindo, ...\\n43    I was made very welcome and the accommodation,...\\n44    The place is small but the room was spacious e...\\n45    My parents and I visited NYC for 9 days. Lisa ...\\n46    LisaRoxanne was very prompt to get in touch an...\\n47    Super séjour à Brooklyn. L'appartement est bie...\\n48    Brilliant host who made us feel welcome withou...\\n49    Lisa was very nice. The apartament was just li...\\n50    This was my first time using Airbnb and it was...\\n51    Brilliant host, clean and comfortable apartmen...\\n52    This was my first Airbnb experience, and she w...\\n53    i had a pleasant stay at Lisa's place. its ver...\\n54    L'appartement est tel que décrit , propre et a...\\n55    Quartier agréable,a 20mn de métro de manhattan...\\n56    Quartier très calme et agréable. L'appartement...\\n57    Dear LisaRoxanne, \\\\nThank you for your hospita...\\n58    Le logement est conforme aux photos et très ag...\\n59    We were 2 weeks to guest at Lisa Roxanne. We l...\\n60    The floor is really nice, clean, very well loc...\\n61    We stayed only for one night and are well rest...\\n62    We had a great stay here! We had all the ameni...\\n63    This place is fantastic, LisaRoxanne was an ex...\\n64    Lisa Roxanne nous a très bien reçus. Elle est ...\\n65    We felt welcome and treated well. Neighbourhoo...\\n66    We had a good time staying at LisaRoxanne's pl...\\n67    LisaRoxanne was at home when we arrived and he...\\n68    Accoglienza buona, le foto promettevano qualco...\\n69    Logement satisfaisant pour decouvrir New York ...\\n70    Great stay at Liz's apartment. The place was c...\\n71    Great place in an authentic and fun NYC-neighb...\\n72    Lisa nous attendait et m'avez indiqué les dire...\\n73    La padrona di casa è stata precisissimo nelle ...\\n74    Nice place with access to Metro bus within fee...\\n75    Everything went perfect. Good room, good WiFi,...\\n76    LisaRoxanne was very accommodating. We like be...\\n77                     Close to subway in leafy street.\\n78    LisaRoxanne met us when we arrived and gave us...\\n79    Alles war supersauber und nett hergerichtet. D...\\n80    We had a great stay at LisaRoxanne's place whe...\\n81    Super great location for our needs\\\\nThe space ...\\n82    Lisa was Awesome and her place was Great. We d...\\n83    Lisa's place is very nice and clean, the neigh...\\n84    My friend and I spent almost 2 weeks at LisaRo...\\n85    Stopped at Here for 5 nights, loved everything...\\n86    Lisa was very helpful and kind. We had all we ...\\n87    A good space, in need of some tender loving ca...\\n88    Great experience! LisaRoxanne is a wonderful, ...\\n89    My boyfriend and I loved staying with LisaRoxa...\\n90    We loved our stay at Lisa's, she was extreamel...\\n91    LisaRoxanne is a warm and helpful host, and th...\\n92            Lisa was very welcoming and accommodating\\n93    Lisa Roxanne was a great and helpful host. We ...\\n94    We loved the place, very clean and private, to...\\n95    We really enjoyed our stay at Lisa Roxannes, w...\\n96    Wir haben sieben Tage bei Lisa Roxanne & ihrer...\\n97    Gute Lage. Sehr guter Preis. Wir konnten bei d...\\n98    LisaRoxanne was very nice and outgoing.  She i...\\n99    Wir haben 8 Nächte in Lisas Unterkunft verbrac...\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =df.comments[1:100].to_string()\n",
    "len(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(tokens):  <class 'list'>\n",
      "type(output):  <class 'list'>\n",
      "type(data_words_nostops):  <class 'list'>\n",
      "type(data_words_bigrams):  <class 'list'>\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "s = df.comments[1:1000].to_string().lower()\n",
    "s = s.replace('\\n', '')\n",
    "s = re.sub(r'[1-9]','', s)\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "\n",
    "# Remove blank comments\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "tokens = remove_stopwords(tokens)\n",
    "print('type(tokens): ', type(tokens))\n",
    "\n",
    "output = list(ngrams(tokens, 2))\n",
    "print('type(output): ', type(output))\n",
    "# print(output, 'output')\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "print('type(data_words_nostops): ', type(data_words_nostops))\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "print('type(data_words_bigrams): ', type(data_words_bigrams))\n",
    "# print(data_words_bigrams)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# ! python3 -m spacy download en\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(str(output), allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.454*\"stay\" + 0.341*\"great\" + 0.188*\"location\" + 0.000*\"family\" + '\n",
      "  '0.000*\"apartment\" + 0.000*\"wonderful\" + 0.000*\"perfect\" + 0.000*\"really\" + '\n",
      "  '0.000*\"enjoy\" + 0.000*\"time\"'),\n",
      " (1,\n",
      "  '0.148*\"nous\" + 0.001*\"avon\" + 0.001*\"be\" + 0.001*\"journ\" + 0.001*\"pass\" + '\n",
      "  '0.001*\"e\" + 0.001*\"longer\" + 0.001*\"long\" + 0.001*\"lixaroxanne\" + '\n",
      "  '0.001*\"pann\"'),\n",
      " (2,\n",
      "  '0.803*\"good\" + 0.000*\"eat\" + 0.000*\"price\" + 0.000*\"safe\" + 0.000*\"review\" '\n",
      "  '+ 0.000*\"apa\" + 0.000*\"know\" + 0.000*\"around\" + 0.000*\"look\" + '\n",
      "  '0.000*\"apartment\"'),\n",
      " (3,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (4,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (5,\n",
      "  '0.938*\"place\" + 0.000*\"family\" + 0.000*\"total\" + 0.000*\"miss\" + '\n",
      "  '0.000*\"wonderful\" + 0.000*\"apartment\" + 0.000*\"perfect\" + 0.000*\"clean\" + '\n",
      "  '0.000*\"gem\" + 0.000*\"enjoy\"'),\n",
      " (6,\n",
      "  '0.009*\"before\" + 0.001*\"cancel\" + 0.001*\"reservation\" + 0.001*\"here\" + '\n",
      "  '0.001*\"once\" + 0.001*\"character\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"midtow\"'),\n",
      " (7,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (8,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (9,\n",
      "  '0.524*\"day\" + 0.117*\"welcoming\" + 0.041*\"helpful\" + 0.000*\"apartment\" + '\n",
      "  '0.000*\"lovely\" + 0.000*\"reservation\" + 0.000*\"befor\" + 0.000*\"really\" + '\n",
      "  '0.000*\"where\" + 0.000*\"list\"'),\n",
      " (10,\n",
      "  '0.066*\"meet\" + 0.001*\"never\" + 0.001*\"actually\" + 0.001*\"friend\" + '\n",
      "  '0.001*\"door\" + 0.001*\"pleasure\" + 0.001*\"person\" + 0.001*\"apartment\" + '\n",
      "  '0.001*\"need\" + 0.001*\"lisaroxanne\"'),\n",
      " (11,\n",
      "  '0.844*\"nice\" + 0.000*\"family\" + 0.000*\"really\" + 0.000*\"hospitable\" + '\n",
      "  '0.000*\"people\" + 0.000*\"confortable\" + 0.000*\"quiet\" + 0.000*\"want\" + '\n",
      "  '0.000*\"give\" + 0.000*\"apartment\"'),\n",
      " (12,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (13,\n",
      "  '0.399*\"welcome\" + 0.164*\"warm\" + 0.164*\"friendly\" + 0.000*\"make\" + '\n",
      "  '0.000*\"gracious\" + 0.000*\"space\" + 0.000*\"clean\" + 0.000*\"mother\" + '\n",
      "  '0.000*\"familial\" + 0.000*\"come\"'),\n",
      " (14,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (15,\n",
      "  '0.925*\"very\" + 0.000*\"family\" + 0.000*\"apartment\" + 0.000*\"really\" + '\n",
      "  '0.000*\"lovely\" + 0.000*\"time\" + 0.000*\"wonderful\" + 0.000*\"maryellen\" + '\n",
      "  '0.000*\"enjoy\" + 0.000*\"feel\"'),\n",
      " (16,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (17,\n",
      "  '0.901*\"host\" + 0.000*\"wonderful\" + 0.000*\"family\" + 0.000*\"amazing\" + '\n",
      "  '0.000*\"flexible\" + 0.000*\"phantastic\" + 0.000*\"fantastic\" + 0.000*\"wonder\" '\n",
      "  '+ 0.000*\"apartment\" + 0.000*\"go\"'),\n",
      " (18,\n",
      "  '0.001*\"davv\" + 0.001*\"situato\" + 0.001*\"old\" + 0.001*\"image\" + '\n",
      "  '0.001*\"situated\" + 0.001*\"situate\" + 0.001*\"posizione\" + 0.001*\"explore\" + '\n",
      "  '0.001*\"ligge\" + 0.001*\"ligging\"'),\n",
      " (19,\n",
      "  '0.825*\"room\" + 0.000*\"little\" + 0.000*\"double\" + 0.000*\"clean\" + '\n",
      "  '0.000*\"big\" + 0.000*\"living\" + 0.000*\"confortable\" + 0.000*\"family\" + '\n",
      "  '0.000*\"price\" + 0.000*\"cosy\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -19.848756660517264\n",
      "\n",
      "Coherence Score:  0.6562155469743248\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristandealwis/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data, kwds)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text/html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[0;32m--> 313\u001b[0;31m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x                   y  topics  cluster       Freq\n",
       "topic                                                                    \n",
       "0      0.426040+0.000000j -0.342410+0.000000j       1        1  45.617077\n",
       "15     0.246893+0.000000j  0.155809+0.000000j       2        1  11.646259\n",
       "5      0.281403+0.000000j  0.269625+0.000000j       3        1  11.494942\n",
       "17     0.191926+0.000000j  0.072534+0.000000j       4        1   8.850121\n",
       "11     0.094545+0.000000j  0.011447+0.000000j       5        1   3.759474\n",
       "2      0.045170+0.000000j -0.003834+0.000000j       6        1   3.731988\n",
       "19     0.070108+0.000000j  0.003076+0.000000j       7        1   3.468595\n",
       "9     -0.049276+0.000000j -0.019566+0.000000j       8        1   2.700602\n",
       "13    -0.024301+0.000000j -0.017196+0.000000j       9        1   2.596212\n",
       "10    -0.118270+0.000000j -0.012732+0.000000j      10        1   0.829091\n",
       "1     -0.120298+0.000000j -0.014528+0.000000j      11        1   0.767534\n",
       "6     -0.116221+0.000000j -0.011471+0.000000j      12        1   0.589153\n",
       "18    -0.115965+0.000000j -0.011344+0.000000j      13        1   0.561681\n",
       "8     -0.115965+0.000000j -0.011344+0.000000j      14        1   0.497674\n",
       "12    -0.115965+0.000000j -0.011344+0.000000j      15        1   0.492862\n",
       "3     -0.115965+0.000000j -0.011344+0.000000j      16        1   0.480807\n",
       "7     -0.115965+0.000000j -0.011344+0.000000j      17        1   0.480747\n",
       "16    -0.115965+0.000000j -0.011344+0.000000j      18        1   0.480700\n",
       "4     -0.115965+0.000000j -0.011344+0.000000j      19        1   0.478364\n",
       "14    -0.115965+0.000000j -0.011344+0.000000j      20        1   0.476129, topic_info=    Category         Freq       Term        Total  loglift  logprob\n",
       "2    Default  1920.000000      place  1920.000000  30.0000  30.0000\n",
       "8    Default  1918.000000       very  1918.000000  29.0000  29.0000\n",
       "12   Default  1420.000000       host  1420.000000  28.0000  28.0000\n",
       "48   Default  3686.000000       stay  3686.000000  27.0000  27.0000\n",
       "27   Default  2770.000000      great  2770.000000  26.0000  26.0000\n",
       "..       ...          ...        ...          ...      ...      ...\n",
       "338  Topic20     0.082499        too     2.201032   2.0633  -6.9344\n",
       "332  Topic20     0.082499      basic     2.201005   2.0633  -6.9344\n",
       "328  Topic20     0.082499        die     2.200737   2.0635  -6.9344\n",
       "326  Topic20     0.082499  enjoyable     2.201165   2.0633  -6.9344\n",
       "325  Topic20     0.082499        lot     2.201065   2.0633  -6.9344\n",
       "\n",
       "[3855 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "17       12  0.329195     before\n",
       "16        8  0.992411        day\n",
       "4         9  0.977195   friendly\n",
       "0         6  0.995485       good\n",
       "27        1  0.999211      great\n",
       "5         8  0.925481    helpful\n",
       "12        4  0.998836       host\n",
       "45        1  0.998614   location\n",
       "31       10  0.846716       meet\n",
       "9         5  0.995495       nice\n",
       "29       11  0.896281       nous\n",
       "2         3  0.998900      place\n",
       "6         7  0.995499       room\n",
       "48        1  0.999311       stay\n",
       "8         2  0.998711       very\n",
       "23        9  0.976506       warm\n",
       "26        9  0.986748    welcome\n",
       "24        8  0.964754  welcoming, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 16, 6, 18, 12, 3, 20, 10, 14, 11, 2, 7, 19, 9, 13, 4, 8, 17, 5, 15])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
